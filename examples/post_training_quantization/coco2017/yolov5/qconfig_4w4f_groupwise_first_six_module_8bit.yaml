BACKEND: virtual
SCHEDULE:
  FUSE_BN: True
W:
  QSCHEME: per-group-affine
  QUANTIZER:
    TYPE: uniform
    BIT: 4
    GROUP_SIZE: 1
  OBSERVER:
    TYPE: MSE
  SPECIFIC: [{
    "model_0_conv": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_24": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_25": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_26": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_1_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_2_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_3_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_4_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_5_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-channel-affine", "QUANTIZER.GROUP_SIZE", -1],
  }]
  BIAS_CORRECTION: True
A:
  QSCHEME: per-channel-affine
  QUANTIZER:
    TYPE: pwlq
    BIT: 4
  OBSERVER:
    TYPE: MSE
    LAYOUT: NCHW
  SPECIFIC: [{
    "model_0_conv": ["QUANTIZER.DISABLE", True, "QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_24": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_25": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_26": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_1_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_2_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_3_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_4_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
    "model_5_*": ["QUANTIZER.BIT", 8, "OBSERVER.TYPE", "MINMAX", "QSCHEME", "per-tensor-affine", "QUANTIZER.GROUP_SIZE", -1],
  }]

